{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal of a recommender system is to increase \"product\" sales. Recommender systems are, after all, utilized by merchants to increase their profits. Although the primary goal of a recommendation system is to increase revenue for the merchant, this is often achieved in ways that are less obvious than might seem at first sight. \n",
    "\n",
    "In order to achieve the broader business-centric goal of increasing revenue, the common operational and technical goals of recommender systems are as follows:\n",
    "1. *Relevance:* \n",
    "   - Recommend items that are relevant to the user.\n",
    "2. *Novelty:* \n",
    "   - Recommend items that the user has not seen in the past.\n",
    "3. *Serendipity:* \n",
    "   - Recommend items that that somewhat \"unexpected\" (i.e. surprising to the user).\n",
    "4. *Recommendation Diversity*:\n",
    "   - Recommend items that are not very similar.\n",
    "\n",
    "Also, providing the user an explanation for why a particular item is recommended is often useful. For example, in the case of Netflix, recommendations are provided along with previously watched movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Types of Recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic models for recommender systems work with two kinds of data, which are:\n",
    "- (i) the user-item interactions, such as ratings or buying behavior.\n",
    "- (ii) the attribute information about the users and items such as textual profiles or relevant keywords.\n",
    "\n",
    "Methods that use the former are referred to as **collaborative filtering** methods, whereas methods that use the latter are referred to as **content-based** recommender methods. Note that content-based systems also use the ratings matrices in most cases, although the model is usually focused on the ratings of a single user rather than those of all users. In **knowledge-based** recommender systems,\n",
    "the recommendations are based on explicitly specified user requirements Instead of using historical rating or buying data, external knowledge bases and constraints are used to create the recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Approach**             | **Conceptual Goal**                                            | **Input**                                    |\n",
    "|----------------------|-------------------------------------------------|------------------------------------------|\n",
    "| **Collaborative Filtering** | Recommend items based on collaborative approach that leverages the ratings and actions of my peers/myself | User ratings + community ratings|\n",
    "| **Content-Based**        | Recommend items based on the content (attributes) I have favored in my past ratings and actions | User ratings + item attributes (e.g., description, genre) |\n",
    "| **Knowledge-based**               | Recommend items based on my explicit specification of the kind of content (attributes) I want | User specification + item attributes + domain knowledge |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Types of Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design of recommendation algorithms is influenced by the system used for tracking ratings (i.e. the user-item interaction). The ratings are often specified on a scale that indicates the specific level of like or dislike of the item at hand. There are two types of feedback as follows:\n",
    "\n",
    "**Type of Feedback:** <br>\n",
    "    - *Explicit* Feedback: Ratings, Likes, Dislikes <br>\n",
    "    - *Implicit* Feedback: Views, Clicks, Purchases <br>\n",
    "\n",
    "These can take the form of:\n",
    "   - *Interval:* 1 to 5 (i.e. 1, 2, 3, 4, 5), -10 to 10 (i.e. -10, -9, ... , 9, 10)\n",
    "   - *Ordinal:* Strongly agree, Agree, Neutral, Disagree, Strongly disagree (difference is that the distant between these is not always equal)\n",
    "   - *Binary:* Like, Dislike (special case of interval and ordinal)\n",
    "   - *Unary:* Allow the users to only specify positive feedback (e.g. \"like\" on facebook)\n",
    "\n",
    "Note that possible ratings might vary with the system at hand. Along each of the possible ratings, we have indicated a semantic interpretation of the user’s level of interest. This interpretation might vary slightly across different merchants, such as Amazon or Netflix. For example, Netflix uses a 5-star ratings system in which the 4-star point corresponds to “really liked it,” and the central 3-star point corresponds to “liked it.” Therefore, there are three favorable ratings and two unfavorable ratings in Netflix, which leads to an unbalanced rating scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborative filtering models** use the collaborative power of the ratings provided by multiple users to make recommendations. The basic idea of collaborative filtering methods is that these unspecified ratings can be imputed because the observed ratings are often highly correlated across various users and items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of methods that are commonly used in collaborative filtering, which are referred to as:\n",
    "1. **Memory-based methods:** Memory-based methods are also referred to as neighborhood-based collaborative filtering algorithms and can be defined in one of two ways:\n",
    "\n",
    "   a. *User-based* collaborative filtering: <br> In this case, the ratings provided by like-minded users of a target user A are used in order to make the recommendations for A. Thus, the basic idea is to determine users, who are similar to the target user A, and recommend ratings for the unobserved ratings of A by computing weighted averages of the ratings of this peer group.\n",
    "\n",
    "   b. *Item-based* collaborative filtering: <br> For example, user's ratings on similar science fiction movies like Alien and Predator can be used to predict his rating on Terminator. Similarity functions are computed between the columns of the ratings matrix to discover similar items.\n",
    "\n",
    "2. **Model-based methods:** In model-based methods, machine learning and data mining methods are used in the context of predictive models. In cases where the model is parameterised, the parameters of this model are learned within the context of an optimization framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next couple of cells, we will explore the famous MovieLens 100K dataset for the purposes of collaborative filtering and implement some of the most popular collaborative filtering techniques to recommend items to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atawua/.local/share/virtualenvs/movielens-100k-recommender-systems-Tq6dyjba/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import common python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import cornac library (for collaborative filtering)\n",
    "from cornac.datasets import movielens\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from http://files.grouplens.org/datasets/movielens/ml-100k/u.data\n",
      "will be cached into /Users/atawua/.cornac/ml-100k/u.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.99MB [00:03, 613kB/s]                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id item_id  rating\n",
       "0     196     242     3.0\n",
       "1     186     302     3.0\n",
       "2      22     377     1.0\n",
       "3     244      51     2.0\n",
       "4     166     346     1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the 100K MovieLens dataset from cornac\n",
    "cornac_ratings = movielens.load_feedback(variant=\"100K\") \n",
    "\n",
    "# Convert to pd.DataFrame for ease of exploration\n",
    "ratings_df = pd.DataFrame(cornac_ratings, columns=[\"user_id\", \"item_id\", \"rating\"])\n",
    "ratings_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique # of users:  943\n",
      "Unique # of items:  1682\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique # of users: \", ratings_df[\"user_id\"].nunique())\n",
    "print(\"Unique # of items: \", ratings_df[\"item_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   user_id  100000 non-null  object \n",
      " 1   item_id  100000 non-null  object \n",
      " 2   rating   100000 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Get information about the ratings data\n",
    "ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0, 4.0, 5.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the type of rating\n",
    "sorted(ratings_df.rating.unique())\n",
    "\n",
    "# NOTE:\n",
    "# [Implicit, Explicit] -> Explicit\n",
    "# [Interval, Ordinal, Binary, Unary] -> Interval (1-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of ratings among items often satisfies a property in real-world settings, which is referred to as the <b>*long-tail*</b> property. According to this property, only a small fraction of the items are rated frequently. Such items are referred to as popular items. And because the vast majority of items are rated rarely, this results in a highly skewed distribution of the underlying ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demonstrate long-tail problem\n",
    "ratings_per_movie = ratings_df.groupby('item_id').size().reset_index(name='count')\n",
    "\n",
    "# Plot the total amount a movie has been rated in descending order\n",
    "plt.figure(figsize=(21, 10))\n",
    "sns.barplot(\n",
    "    x=\"item_id\",\n",
    "    y=\"count\",\n",
    "    data=ratings_per_movie, \n",
    "    order=ratings_per_movie.sort_values(\"count\", ascending=False)[\"item_id\"]\n",
    "    )\n",
    "plt.title(\"Long Tail of Movie Ratings\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# NOTE:\n",
    "# Capture what the long tail problem is and how it affects collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, the high-frequency items tend to be relatively competitive items with little profit for the merchant. On the other hand, the lower frequency items have larger profit margins. In such cases, it may be advantageous to the merchant to recommend lower frequency items. In fact, analysis suggests that many companies, such as Amazon.com, make most of their profit by selling items in the long tail.\n",
    "\n",
    "Because of the rarity of observed ratings in the long tail it is generally more difficult to provide robust rating predictions in the long tail. In fact, many recommendation algorithms have a tendency to suggest popular items rather than infrequent items. This phenomenon also has a **negative impact on diversity** and users may often become bored by receiving the same set of recommendations of popular items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>196</td>\n",
       "      <td>393</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>196</td>\n",
       "      <td>381</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>196</td>\n",
       "      <td>251</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>196</td>\n",
       "      <td>655</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>196</td>\n",
       "      <td>67</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>196</td>\n",
       "      <td>306</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7517</th>\n",
       "      <td>196</td>\n",
       "      <td>238</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7842</th>\n",
       "      <td>196</td>\n",
       "      <td>663</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>196</td>\n",
       "      <td>111</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10254</th>\n",
       "      <td>196</td>\n",
       "      <td>580</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10981</th>\n",
       "      <td>196</td>\n",
       "      <td>25</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13733</th>\n",
       "      <td>196</td>\n",
       "      <td>286</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14606</th>\n",
       "      <td>196</td>\n",
       "      <td>94</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16834</th>\n",
       "      <td>196</td>\n",
       "      <td>692</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17102</th>\n",
       "      <td>196</td>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17830</th>\n",
       "      <td>196</td>\n",
       "      <td>428</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18853</th>\n",
       "      <td>196</td>\n",
       "      <td>1118</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>196</td>\n",
       "      <td>70</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22271</th>\n",
       "      <td>196</td>\n",
       "      <td>66</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22773</th>\n",
       "      <td>196</td>\n",
       "      <td>257</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23189</th>\n",
       "      <td>196</td>\n",
       "      <td>108</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24030</th>\n",
       "      <td>196</td>\n",
       "      <td>202</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25726</th>\n",
       "      <td>196</td>\n",
       "      <td>340</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32721</th>\n",
       "      <td>196</td>\n",
       "      <td>287</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33536</th>\n",
       "      <td>196</td>\n",
       "      <td>116</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35197</th>\n",
       "      <td>196</td>\n",
       "      <td>382</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36281</th>\n",
       "      <td>196</td>\n",
       "      <td>285</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41539</th>\n",
       "      <td>196</td>\n",
       "      <td>1241</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42384</th>\n",
       "      <td>196</td>\n",
       "      <td>1007</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50147</th>\n",
       "      <td>196</td>\n",
       "      <td>411</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52726</th>\n",
       "      <td>196</td>\n",
       "      <td>153</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56628</th>\n",
       "      <td>196</td>\n",
       "      <td>13</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59165</th>\n",
       "      <td>196</td>\n",
       "      <td>762</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59607</th>\n",
       "      <td>196</td>\n",
       "      <td>173</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60199</th>\n",
       "      <td>196</td>\n",
       "      <td>1022</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60706</th>\n",
       "      <td>196</td>\n",
       "      <td>845</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78787</th>\n",
       "      <td>196</td>\n",
       "      <td>269</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87863</th>\n",
       "      <td>196</td>\n",
       "      <td>110</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id item_id  rating\n",
       "0         196     242     3.0\n",
       "940       196     393     4.0\n",
       "1133      196     381     4.0\n",
       "1812      196     251     3.0\n",
       "1896      196     655     5.0\n",
       "2374      196      67     5.0\n",
       "6910      196     306     4.0\n",
       "7517      196     238     4.0\n",
       "7842      196     663     5.0\n",
       "10017     196     111     4.0\n",
       "10254     196     580     2.0\n",
       "10981     196      25     4.0\n",
       "13733     196     286     5.0\n",
       "14606     196      94     3.0\n",
       "16834     196     692     5.0\n",
       "17102     196       8     5.0\n",
       "17830     196     428     4.0\n",
       "18853     196    1118     4.0\n",
       "21605     196      70     3.0\n",
       "22271     196      66     3.0\n",
       "22773     196     257     2.0\n",
       "23189     196     108     4.0\n",
       "24030     196     202     3.0\n",
       "25726     196     340     3.0\n",
       "32721     196     287     3.0\n",
       "33536     196     116     3.0\n",
       "35197     196     382     4.0\n",
       "36281     196     285     5.0\n",
       "41539     196    1241     3.0\n",
       "42384     196    1007     4.0\n",
       "50147     196     411     4.0\n",
       "52726     196     153     5.0\n",
       "56628     196      13     2.0\n",
       "59165     196     762     3.0\n",
       "59607     196     173     2.0\n",
       "60199     196    1022     4.0\n",
       "60706     196     845     4.0\n",
       "78787     196     269     3.0\n",
       "87863     196     110     1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the example of a random user (i.e. user 196)\n",
    "ratings_df[ratings_df[\"user_id\"] == \"196\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>item_id</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>1001</th>\n",
       "      <th>1002</th>\n",
       "      <th>1003</th>\n",
       "      <th>1004</th>\n",
       "      <th>1005</th>\n",
       "      <th>1006</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943 rows × 1682 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "item_id    1   10  100  1000  1001  1002  1003  1004  1005  1006  ...  990  \\\n",
       "user_id                                                           ...        \n",
       "1        5.0  3.0  5.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  NaN   \n",
       "10       4.0  NaN  5.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  NaN   \n",
       "100      NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  3.0   \n",
       "101      3.0  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  NaN   \n",
       "102      3.0  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  NaN   \n",
       "...      ...  ...  ...   ...   ...   ...   ...   ...   ...   ...  ...  ...   \n",
       "95       5.0  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  NaN   \n",
       "96       5.0  NaN  5.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  NaN   \n",
       "97       4.0  NaN  2.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  NaN   \n",
       "98       NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  NaN   \n",
       "99       4.0  NaN  5.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  NaN   \n",
       "\n",
       "item_id  991  992  993  994  995  996  997  998  999  \n",
       "user_id                                               \n",
       "1        NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "10       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "100      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "101      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "102      NaN  NaN  2.0  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "95       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "96       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "97       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "98       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "99       NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[943 rows x 1682 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the utility matrix\n",
    "ratings_matrix = ratings_df.pivot_table(index='user_id', columns='item_id', values='rating', fill_value=np.NaN)\n",
    "ratings_matrix\n",
    "\n",
    "# NOTE:\n",
    "# The NaN values represent movies that the user has not rated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of subsequent discussion, we assume that the user-item ratings matrix (i.e. ratings_matrix)\n",
    "is an incomplete $m × n$ matrix $R = [r_{uj}]$ containing $m=943$ users and $n=1682$ items.\n",
    "\n",
    "We can see that only a small subset of the ratings matrix is specified or observed. Our goal would be to understand which items (i.e. movies) we should recommend to the users that would be likely to see. Like all other collaborative filtering algorithms, neighborhood-based collaborative filtering algorithms can be formulated in one of two ways to address the the recommendation problem:\n",
    "1. Predicting the rating value of a user-item combination.\n",
    "2. Determining the top-k items or top-k users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, in the context of recommender systems, especially user-based collaborative filtering, sparsity refers to the proportion of missing ratings in the user-item interaction matrix. For a robust collaborative filtering model to work a dense matrix would be needed so that we can use the collaborative power of other users. In other words, with a sparse matrix user \"similarities\" might be unreliable and there would be a low coverage of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix sparsity: 93.70%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sparsity of the ratings matrix\n",
    "total_elements = ratings_matrix.size\n",
    "non_nan_elements = np.count_nonzero(~np.isnan(ratings_matrix))\n",
    "density = non_nan_elements / total_elements\n",
    "\n",
    "print(\"Matrix sparsity: {:.2%}\".format(1 - density)) # sparsity = (# ratings) / ( total # of elements)\n",
    "\n",
    "# NOTE:\n",
    "# Sparsity is a measure of how much of the matrix is empty (i.e. not rated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no universally \"good\" level of sparsity, as it depends on the specific dataset and the methods used for recommendation. However, some general guidelines suggest that datasets with sparsity levels below 80% are considered dense enough to perform well with user-based collaborative filtering. On the other hand, Datasets with sparsity levels above 90% are considered sparse. This high level of sparsity can pose significant challenges for user-based collaborative filtering methods.\n",
    "\n",
    "For the purposes of the notebook, we won't be bothered to much about the sparsity as the main goal is to explore the mathematics and underlying properties of the different collaborative filtering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Memory-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-Based Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cornac library (for collaborative filtering)\n",
    "import cornac\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.models import UserKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the memory-based collaborative filtering approach, user-based neighborhoods are defined in order to identify similar users to the _target_ user for whom the rating predictions are being computed. \n",
    "\n",
    "For the $m × n$ ratings matrix $R = [r_{uj}]$ with $m$ users and $n$ items, let $I_u$ denote the set of item indices for which ratings have been specified by user (row) $u$. The problem at hand is to predict the user's rating for item $j$.\n",
    "\n",
    "The most common **predictive function** of the neighborhood-based predictive function is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{r}_{uj} = \\mu_u + \\frac{ \\sum\\limits_{v \\in P^{k}_u(j)} \\text{Sim}(u, v) \\cdot (r_{vj} - \\mu_v)} {\\sum\\limits_{v \\in P^{k}_u(j)} \\text{Sim}(u, v)} = \\mu_u + \\frac{ \\sum\\limits_{v \\in P^{k}_u(j)} \\text{Sim}(u, v) \\cdot s_{vj}} {\\sum\\limits_{v \\in P^{k}_u(j)} \\text{Sim}(u, v)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "- $\\hat{r}_{uj}$ indicates a predicted rating of user $u$ for item $j$.\n",
    "- $\\mu_u$ being the average rating of the user $u$.\n",
    "- $r_{uj} $ being the rating of user $u$ for item $j$.\n",
    "- $ P_u(j) = $ be the set of $k$ closest users to target user $u$, who have specified ratings for item $ j $.\n",
    "- $\\text{Sim}(u, v)$ being the similarity of target user $u$ with user $v$.\n",
    "- $s_{vj} = r_{vj} - \\mu_v, \\space\\space \\forall v \\in \\{1, \\dots, m\\}$.\n",
    "\n",
    "The average rating $\\mu_u$ of the user is calculated as:\n",
    "$$ \\mu_u = \\frac{\\Sigma_{k \\in \\mathcal{I}_u} r_{uk}}{|\\mathcal{I}_u|} \\ \\ \\forall u \\in \\{1, \\dots, m\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡The reason why we substract the mean of each user with their ratings (i.e. standarise the ratings) is that different users may provide ratings on different scales. One user might rate all items highly, whereas another user might rate all items negatively. The raw ratings, therefore, need to be mean-centered in row-wise fashion, before determining the (weighted) average rating of the peer group. The mean-centered rating $s_{uj}$ of a user $u$ for item $j$ is defined by subtracting her mean rating from the raw rating $r_{uj}$ as shown above.💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Average rating of user 196: 3.62\n",
      "# Average rating of user 350: 4.32\n"
     ]
    }
   ],
   "source": [
    "# Average rating of user 196\n",
    "print(\"# Average rating of user 196:\", round(ratings_df[ratings_df[\"user_id\"] == \"196\"][\"rating\"].mean(), 2))\n",
    "\n",
    "# Average rating of user 350\n",
    "print(\"# Average rating of user 350:\", round(ratings_df[ratings_df[\"user_id\"] == \"350\"][\"rating\"].mean(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all the components of the predictive function are known apart from the $\\text{Sim}(u,v)$ which is not specified yet. In order to determine the neighborhood of the target user $u$, their **similarity** to all the other users needs to computed. \n",
    "\n",
    "There are different similarities that can be used with each one having their pros and cons. An example of popular similarities are the Pearson similarity and the Raw Cosine similarity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathrm{Sim}(u,v) = \\mathrm{Pearson}(u,v) &= \\frac{\\Sigma_{k \\in \\mathcal{I}_u \\cap \\mathcal{I}_v} (r_{uk} - \\mu_u) * (r_{vk} - \\mu_v)}{\\sqrt{\\Sigma_{k \\in \\mathcal{I}_u \\cap \\mathcal{I}_v} (r_{uk} - \\mu_u)^2} * \\sqrt{\\Sigma_{k \\in \\mathcal{I}_u \\cap \\mathcal{I}_v} (r_{vk} - \\mu_v)^2}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathrm{Sim}(u,v) = \\mathrm{RawCosine}(u,v) &= \\frac{\\Sigma_{k \\in \\mathcal{I}_u \\cap \\mathcal{I}_v} r_{uk} * r_{vk}}{\\sqrt{\\Sigma_{k \\in \\mathcal{I}_u \\cap \\mathcal{I}_v} r_{uk}^2} * \\sqrt{\\Sigma_{k \\in \\mathcal{I}_u \\cap \\mathcal{I}_v} r_{vk}^2}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reliability of the similarity function $\\text{Sim}(u, v)$ is often affected by the number of common ratings $|I_u ∩ I_v|$ between users $u$ and $v$. <br>\n",
    "**Note:** The Pearson correlation coefficient is preferable to the raw cosine because of the bias adjustment effect of mean-centering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build our model, we will split our data into train and test set similarly to traditional ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE=False\n",
    "SEED=42\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_test_ratio_split = RatioSplit(\n",
    "    cornac_ratings, \n",
    "    test_size=0.2,       # Use 20% of the data for testing.\n",
    "    rating_threshold=3,  # Only items with ratings >= 3 are considered as relevant. (recall unbalanced rating scale)\n",
    "    seed=SEED,\n",
    "    verbose=VERBOSE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train & Test in Recommender Systems](img/train-test-ml-recsys.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics for the collaborative filtering model\n",
    "METRICS = [\n",
    "    cornac.metrics.RMSE(),\n",
    "    cornac.metrics.MAE(),\n",
    "    cornac.metrics.Precision(k=10), # Precision @k = (# of relevant movies recommended) / k\n",
    "    cornac.metrics.Recall(k=10),    # Recall @k = (# of relevant movies recommended) / (# of relevant movies)\n",
    "    cornac.metrics.NDCG(k=10),\n",
    "    cornac.metrics.MAP(),\n",
    "    ]\n",
    "\n",
    "# NOTE:\n",
    "# Relevant movies are those with ratings >= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neighbors (i.e. look 50 \"nearest\" users)\n",
    "K=50\n",
    "\n",
    "# Define k-nn models with different similarity metrics\n",
    "uknn_pearson = UserKNN(\n",
    "    k=K, similarity=\"pearson\", mean_centered=True, name=\"UserKNN-pearson\", verbose=VERBOSE\n",
    ")\n",
    "\n",
    "uknn_adjusted_cosine = UserKNN(\n",
    "    k=K, similarity=\"cosine\", mean_centered=True, name=\"UserKNN-adjusted-cosine\", verbose=VERBOSE\n",
    ")\n",
    "\n",
    "uknn_raw_cosine = UserKNN(\n",
    "    k=K, similarity=\"cosine\", mean_centered=False, name=\"UserKNN-raw-cosine\", verbose=VERBOSE\n",
    ")\n",
    "\n",
    "# NOTE:\n",
    "# The number of K was selected randomly as the determination of the best K is out of the scope of this notebook. \n",
    "# However, the best K can be determined using hyperparameter tuning techniques with cross-validation similarly to traditional ML techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "                        |    MAE |   RMSE |    MAP | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "----------------------- + ------ + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "UserKNN-pearson         | 0.7511 | 0.9196 | 0.0227 |  0.0004 |       0.0003 |    0.0002 |    0.0588 |   3.7701\n",
      "UserKNN-adjusted-cosine | 0.7511 | 0.9196 | 0.0227 |  0.0004 |       0.0003 |    0.0002 |    0.0388 |   3.7928\n",
      "UserKNN-raw-cosine      | 0.7616 | 0.9278 | 0.0251 |  0.0004 |       0.0005 |    0.0004 |    0.0393 |   3.7907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute an experiment to evaluate the models on the pre-specified metrics\n",
    "experiment = cornac.Experiment(\n",
    "    eval_method=train_test_ratio_split,\n",
    "    models=[uknn_pearson, uknn_adjusted_cosine, uknn_raw_cosine],\n",
    "    metrics=METRICS,\n",
    ")\n",
    "experiment.run()\n",
    "\n",
    "# NOTE:\n",
    "# Generally, the adjusted cosine similarity is used in item-based collaborative filtering as the average rating of each item (and not user) in the ratings matrix is subtracted from each rating.\n",
    "# However, in user-based collaborative filtering, the mean-centered cosine similarity is used as the average rating of each user is subtracted from each rating (hence the same results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the beginning, the distribution of ratings typically shows a **long-tail** distribution in many real scenarios. Some movies may be very popular and they may repeatedly occur as commonly rated items by different users.\n",
    "\n",
    "The negative impact of these recommendations can be experienced both during the peer group computation and also during the prediction computation. This notion is similar in principle to the deterioration in retrieval quality caused by popular and noninformative words (e.g., “a,” “an,” “the”) in document retrieval applications. Therefore, the proposed solutions used in collaborative filtering are also similar to those used in the information retrieval literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the notion of Inverse Document Frequency (idf) exists in the information retrieval, one can use the notion of Inverse User Frequency as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ w_j = \\log \\bigg( \\frac{m}{m_j} \\bigg), \\space \\forall j \\in \\{1, \\dots n\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "- $ m= $ total number of users.\n",
    "- $ m_j = $ total number of ratings of item $ j $.\n",
    "- $ n= $ total number of items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the Pearson similarity can be rewritten using the idf weights creating the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\n",
    "\\mathrm{Pearson}(u,v) &= \\frac{\\Sigma_{k \\in \\mathcal{I}_u \\cap \\mathcal{I}_v} w_{k}(r_{uk} - \\mu_u) * (r_{vk} - \\mu_v)}{\\sqrt{\\Sigma_{k \\in \\mathcal{I}_u \\cap \\mathcal{I}_v} w_{k}(r_{uk} - \\mu_u)^2} * \\sqrt{\\Sigma_{k \\in \\mathcal{I}_u \\cap \\mathcal{I}_v} w_{k}(r_{vk} - \\mu_v)^2}}\n",
    "\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create idf weighting model using the pearson similarity metric\n",
    "uknn_idf = UserKNN(\n",
    "  k=K, similarity=\"pearson\", weighting=\"idf\", name=\"UserKNN-IDF\", verbose=VERBOSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "                   |    MAE |   RMSE |    MAP | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "------------------ + ------ + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "UserKNN-pearson    | 0.7511 | 0.9196 | 0.0227 |  0.0004 |       0.0003 |    0.0002 |    0.0454 |   3.8609\n",
      "UserKNN-raw-cosine | 0.7616 | 0.9278 | 0.0251 |  0.0004 |       0.0005 |    0.0004 |    0.0400 |   3.7682\n",
      "UserKNN-IDF        | 0.7519 | 0.9205 | 0.0219 |  0.0004 |       0.0003 |    0.0002 |    0.0392 |   3.7428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute an experiment to evaluate the models on RMSE and MAE\n",
    "experiment_with_idf = cornac.Experiment(\n",
    "    eval_method=train_test_ratio_split,\n",
    "    models=[uknn_pearson, uknn_raw_cosine, uknn_idf],\n",
    "    metrics=METRICS,\n",
    ")\n",
    "experiment_with_idf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, it seems that IDF weighting does not perform better than the pearson correlation. A possible explanation can be that, even though IDF weighting gives more weight to less common items, if these items are not necessarily better indicators of user preferences but are simply less interacted with, this can lead to misleading similarity calculations. As a result, this can lead to an overemphasis on rare items which then leads to recommending items that are not truly relevant, reducing precision and recall.\n",
    "\n",
    "It's generally a good idea to experiment with and without IDF weighting to see if it improves your specific recommendation task. In many cases, incorporating such weighting schemes can lead to more personalized and accurate recommendations as it decreases popularity bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, memory-based collaborative filtering, like any other traditional ML techniques, come with their advantages and disadvantages:\n",
    "- **Pros**:\n",
    "  1. Easy to implement.\n",
    "  2. Easy to explain.\n",
    "\n",
    "- **Cons**:\n",
    "  1. Don't work well with sparse data (i.e. can't find enough users to \"collaborate\").\n",
    "  2. Can be biased towards popular items reduce diversity and novelty. (i.e. **Popularity Bias**).\n",
    "  3. Perform poorly with new users or items (**cold start problem**) because there is not enough interaction data to calculate similarities effectively. In such cases, content-based and knowledge-based methods are more preferably as they are more robust than collaborative models in the presence of **cold starts**.\n",
    "  4. Don't scale well with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Model-based collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood-based methods seen above are generalizations of instance-based learning methods or lazy learning methods in which the prediction approach is specific to the instance being predicted. For example, in user-based neighborhood methods, the peers of the target user are determined in order to perform the prediction.\n",
    "\n",
    "In **model-based methods**, a summarized model of the data is created up front, as with supervised or unsupervised machine learning methods. Therefore, the training (or model building phase) is clearly separated from the prediction phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular techniques of model-bases collaborative filtering is **Matrix Factorisation**. Factorisation is, in fact, a more general way of approximating a matrix (e.g. a ratings matrix) when it is prone to dimensionality reduction because of correlations between columns (or rows). The basic idea of dimensionality reduction methods is to rotate the axis system, so that pairwise correlations between dimensions are removed. Because of the inherent redundancies in the data, the fully specified low rank approximation can be determined even with a small subset of the entries in the original matrix. This fully-specified low rank approximation often provides a robust estimation of the missing entries.\n",
    "\n",
    "In real life, the rating matrix we observe is very sparse. If we then assume that most matrices have a low-rank representation (it can be reconstructed from relatively few basis vectors), then one promising approach is to conduct matrix factorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡The key differences among various matrix factorization methods arise in terms of the constraints imposed on U and V (e.g., orthogonality or non-negativity of the latent vectors) and the nature of the objective function (e.g., minimizing the Frobenius norm or maximizing the likelihood estimation in a generative model). These differences play a key role in the usability of the matrix factorization model in various real-world scenarios.💡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [a] Singular Value Decomposition (SVD) in Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Singular value decomposition (SVD)** in classical linear algebra a form of matrix factorisation in which the _columns of $U$ and $V$ are constrained to be mutually orthogonal_. Mutual orthogonality has the advantage that the concepts can be completely independent of one another, and they can be geometrically interpreted in scatterplots. However, the semantic interpretation of such a decomposition is generally more difficult, because these latent vectors contain both positive and negative quantities, and are constrained by their orthogonality to other concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SVD, one can approximately factorise the ratings matrix $R$ by using truncated SVD of rank $k ≪ min\\{m, n\\}$. <br>\n",
    "For any real rating matrix $ R \\in \\mathbb{R}^{M \\times N} $, SVD seeks the following decomposition:\n",
    "\n",
    "$$ R = Q_k \\Sigma_k P_{k}^{T} $$\n",
    "\n",
    "Here, $Q_k$, $\\Sigma_k$, and $P_k$ are matrices of size $m×k$, $k×k$, and $n×k$, respectively. However,the diagonal matrix $\\Sigma_k$ can be absorbed in either the user factors $Q_k$ or the item factors $P_k$. By convention, the user factors and item factors are defined as follows:\n",
    "\n",
    "- $ U = Q_k \\Sigma_k $\n",
    "- $ V = P_k $\n",
    "\n",
    "Therefore, the factorisation of the ratings matrix $R$ is defined as $R = UV^{T}$. As long as the user and item factor matrices have orthogonal columns, it is easy to convert the resulting factorization into a form that is compliant with SVD.\n",
    "\n",
    "SVD can be formulated, in general, as the following optimisation problem over the matrices $U$ and $V$:\n",
    "\n",
    "$$ \\text{Minimise} \\space J = \\frac{1}{2} ||R - UV^T||^2 $$\n",
    "\n",
    "subject to:\n",
    "- Columns of $U$ are mutually orthogonal.\n",
    "- Columns of $V$ are mutually orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [b] Singular Value Decomposition (SVD) in Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In collaborative filtering, the goal is to approximate the user-item rating matrix $R$ by factorizing it into two lower-rank matrices $U$ and $V$, such that $R≈UV^T$. The optimisation methods (such as Stochastic Gradient Descent or Alternating Least Squares) focus on minimizing the prediction error on the observed entries, not on maintaining orthogonality. Also, the SVD implementation of collaborative filtering includes bias terms for users and items to account for systematic rating deviations. The inclusion of these biases alters the structure of the factorization compared to the traditional SVD, making orthogonality less relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the optimisation problem (with regularisation) that is being solved in the SVD implementation is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Minimize } J = \\frac{1}{2} \\sum_{(u, i) \\in \\mathcal{S}} \\left( R_{ui} - \\mu - b_u - b_i - U_u \\cdot V_i \\right)^2 + \\frac{\\lambda}{2} \\left( \\sum_{u=1}^{M} \\|U_u\\|^2 + \\sum_{i=1}^{N} \\|V_i\\|^2 + \\sum_{u=1}^{M} b_u^2 + \\sum_{i=1}^{N} b_i^2 \\right) $$ \n",
    "\n",
    "where:\n",
    "- $\\mathcal{S}$ be the set of specified entries in the ratings matrix.\n",
    "- $ \\mu $ is the global average rating.\n",
    "- $ b_u $ is the bias of user $u$.\n",
    "- $ b_i $ is the bias of item $i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation problem in order to learn $b_u$, $b_i$, $U_v$ and $V_i$ can traditionally be solved using the following methods:\n",
    "  - Stochastic Gradient Descent (SGD)\n",
    "  - Alternating Least Squares (ALS)\n",
    "  - etc.\n",
    "\n",
    "Each of these algorithms has its own strengths and trade-offs, but **SGD remains a popular choice** due to its advantages:\n",
    "  - Scalability: SGD can handle large-scale data efficiently, making it suitable for large recommender system datasets.\n",
    "  - Simplicity: The algorithm is relatively simple to implement and understand.\n",
    "  - Effectiveness: Despite its simplicity, SGD is highly effective at finding good approximations of the user and item latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.models import SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "                |    MAE |   RMSE |    MAP | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "--------------- + ------ + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "UserKNN-pearson | 0.7511 | 0.9196 | 0.0227 |  0.0004 |       0.0003 |    0.0002 |    0.0591 |   3.7616\n",
      "UserKNN-IDF     | 0.7519 | 0.9205 | 0.0219 |  0.0004 |       0.0003 |    0.0002 |    0.0404 |   3.7738\n",
      "UserSVD         | 0.7334 | 0.8926 | 0.0568 |  0.0924 |       0.0840 |    0.0379 |    0.3107 |   0.5046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_ITER=20\n",
    "LEARNING_RATE=0.01\n",
    "LAMDA_REG=0.02\n",
    "\n",
    "# Define the SVD model with regularisation\n",
    "svd = SVD(\n",
    "    k=K, max_iter=MAX_ITER, learning_rate=LEARNING_RATE, lambda_reg=LAMDA_REG, verbose=VERBOSE, name=\"UserSVD\"\n",
    ")\n",
    "\n",
    "# Compare SVD (model-based) with Pearson and IDF models (memory-based)\n",
    "cornac.Experiment(\n",
    "    eval_method=train_test_ratio_split,\n",
    "    models=[uknn_pearson, uknn_idf, svd],\n",
    "    metrics=METRICS\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can witness that the SVD model performs the best compared to the previous memory based approaches highlighting the robustness of model-based techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory-based collaborative filtering techniques are powerful and here are some advantages and disadvantages that people should be aware of:\n",
    "- **Pros**:\n",
    "  1. Scalability.\n",
    "  2. Sparsity Handling.\n",
    "  3. Improved Accuracy.\n",
    "  4. Reduction of Overfitting.\n",
    "  5. Latent Factors Insight.\n",
    "\n",
    "- **Cons**:\n",
    "  1. Model-based approaches are often more complex and computationally intensive than memory-based methods.\n",
    "  2. Model-based methods often involve multiple hyperparameters that need to be carefully tuned, such as the number of latent factors, regularization parameters, and learning rates.\n",
    "  3. Similar to other collaborative filtering methods, model-based approaches suffer from the cold start problem. New users and items with no interaction history pose a challenge for making accurate recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different factorisation techniques that can be used. The following table is a generic breakdown (not exhaustive) of these techniques along with their Advantages and Disadvantages.\n",
    "\n",
    "| Method          | Constraints     | Objective                     | Advantages/Disadvantages                                                                                              |\n",
    "|-----------------|-----------------|-------------------------------|----------------------------------------------------------------------------------------------------------------------|\n",
    "| Unconstrained   | No constraints  | Frobenius + regularizer       | Highest quality solution<br>Good for most matrices<br>Regularization prevents overfitting<br>Poor interpretability  |\n",
    "| SVD             | Orthogonal Basis| Frobenius + regularizer       | Good visual interpretability<br>Out-of-sample recommendations<br>Good for dense matrices<br>Poor semantic interpretability<br>Suboptimal in sparse matrices |\n",
    "| NMF             | Non-negativity  | Frobenius + regularizer       | Good quality solution<br>High semantic interpretability<br>Loses interpretability with both like/dislike ratings<br>Less overfitting in some cases |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
