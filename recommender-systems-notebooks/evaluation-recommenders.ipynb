{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evaluation of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **three (3)** primary types of evaluation of recommender systems, corresponding to **[1] user studies**, **[2] online evaluations**, and **[3] offline evaluations** with historical data set.\n",
    "\n",
    "* **User Studies:** <br>\n",
    "In user studies, test subjects are actively recruited, and they are asked to interact with the recommender system to perform specific tasks. Feedback can be collected from the user before and after the interaction, and the system also collects information about their interaction with the recommender system. An important advantage of user studies is that they allow for the collection of information about the user interaction with the system. On the other hand, the active awareness of the user about the testing of the recommender system can often bias their choices and actions. It is also difficult and expensive to recruit large cohorts of users for evaluation purposes. In many cases, the recruited users are not representative of the general population because the recruitment process is itself a bias-centric filter, which cannot be fully controlled.\n",
    "\n",
    "* **Online Evaluation:** <br>\n",
    "Online evaluations also leverage user studies except that the users are often real users in a fully deployed or commercial system. This approach is sometimes less susceptible to bias from the recruitment process, because the users are often directly using the system in the natural course of affairs. Typically, users can be sampled randomly, and the various algorithms can be tested with each sample of users. A typical example of a metric, which is used to measure the effectiveness of the recommender system on the users, is the conversion rate. The main disadvantage is that such systems cannot be realistically deployed unless a large number of users are already enrolled. Therefore, it is hard to use this method during the start up phase.\n",
    "\n",
    "* **Offline Evaluation:** <br>\n",
    "In offline testing, historical data, such as ratings, are used. In some cases, temporal information may also be associated with the ratings, such as the timestamp at which each user has rated the item. The main disadvantage of offline evaluations is that they do not measure the actual propensity of the user to react to the recommender system in the future. For example, the data might evolve over time, and the current predictions may not reflect the most appropriate predictions for the future. Furthermore, measures such as accuracy do not capture important characteristics of recommendations, such as serendipity and novelty. Offline methods continue to be the most widely accepted techniques for recommender system evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. General Goals of Evaluation Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from the well known goal of **accuracy**, other general goals of evaluating a recommender system holistically include factors such as **diversity**, **serendipity**, **novelty**, **robustness**, and **scalability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the most general case, ratings are numeric quantities that need to be estimated. Therefore, the accuracy metrics are often similar to those used in regression modeling. Let $R$ be the ratings matrix in which $r_{uj}$ is the known rating of user $u$ for item $j$. Consider the case where a recommendation algorithm estimates this rating as $\\hat{r}_{uj}$. Then, the entry-specific error of the estimation is given by the quantity $\\epsilon_{uj} = \\hat{r}_{uj} − r_{uj}$. \n",
    "\n",
    "The overall error is computed by averaging the entry-specific errors either in terms of absolute values or in terms of squared values.\n",
    "\n",
    "* $\\text{MSE} = \\frac{\\sum_{(u, j) \\in E} e_{uj}^{2}}{|E|}$\n",
    "\n",
    "* $\\text{RMSE} = \\sqrt{\\frac{\\sum_{(u, j) \\in E} e_{uj}^{2}}{|E|}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many recommender systems do not directly estimate ratings; instead, they provide estimates of the underlying ranks. Depending on the nature of the ground-truth, one can use rank-correlation measures, utility-based measures, or the receiver operating characteristic which will be discussed later.\n",
    "\n",
    "Some measures of accuracy are also designed to maximize the profit for the merchant because all items are not equally important from the perspective of the recommendation process. These metrics incorporate item-specific costs into the computation. The main problem with accuracy metrics is that they often do not measure the true effectiveness of a recommender system in real settings. For example, an obvious recommendation might be accurate, but a user might have eventually bought that item anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when a recommender system is highly accurate, it may often not be able to ever recommend a certain proportion of the items, or it may not be able to ever recommend to a certain proportion of the users. This measure is referred to as **Coverage**. This limitation of recommender systems is an artifact of the fact that ratings matrices are sparse. Therefore, the trade-off between accuracy and coverage always needs to be incorporated into the evaluation process. \n",
    "\n",
    "There are two types of coverage, which are referred to as **[1] user-space** coverage and **[2] item-space** coverage, respectively.\n",
    "\n",
    "* **User-Space Coverage:** <br>\n",
    "**User-space** coverage measures the fraction of users for which at least $k$ ratings may be predicted. The value of $k$ should be set to the expected size of the recommendation list. When fewer than $k$ ratings can be predicted for a user, it is no longer possible to present a meaningful recommendation list of size $k$ to the user. Such a situation could occur when a user has specified very few ratings in common with other users. It is difficult to robustly compute the peers of that user, because of very few mutually specified ratings with other users. Therefore, it is often difficult to make sufficient recommendations for that user. For very high levels of sparsity, it is possible that no algorithm may be able to predict even one rating for that user. \n",
    "\n",
    "*Note:* A tricky aspect of user-space coverage is that any algorithm can provide full coverage by simply predicting random ratings for user-item combinations, whose ratings it cannot reliably predict. Therefore, *user-space coverage should always be evaluated in terms of the trade-off between accuracy and coverage.*\n",
    "\n",
    "* **Item-Space Coverage:** <br>\n",
    "**Item-space** coverage measures the fraction of items for which the ratings of at least $k$ users can be predicted. In practice, however, this notion is rarely used, because recommender systems generally provide recommendation lists for users, and they are only rarely used for generating recommended users for items. \n",
    "\n",
    "A different form of item-space coverage evaluation is defined by the notion of **catalog coverage (CC)**, which is specifically suited to recommendation lists. Let $T_u$ represent the list of top-$k$ items recommended to user $u ∈ \\{1, . . .,m\\}$. The catalog coverage $CC$ is defined as the fraction of items that are recommended to at least one user. $$ CC = \\frac{|\\bigcup_{u=1}^{m}T_u|}{n} $$\n",
    "\n",
    "Here, the notation $n$ represents the number of items. It is easy to estimate this fraction through the use of experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Novelty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **novelty** of a recommender system evaluates the likelihood of a recommender system to give recommendations to the user that they are not aware of, or that they have not seen before. Unseen recommendations often increase the ability of the user to discover important insights into their likes and dislikes that they did not know previously. This is more important than discovering items that they were already aware of but they have not rated. \n",
    "\n",
    "The most natural way of measuring novelty is through online experimentation in which users are explicitly asked whether they were aware of an item previously and, fortunately, it is also possible to approximately estimate novelty with offline methods, as long as timestamps are available with the ratings. The basic idea is that novel systems are better at recommending items that are more likely to be selected by the user in the future, rather than at the present time. Therefore, all ratings that were created after a certain point in time $t_0$ are removed from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Serendipity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word “serendipity” literally means “lucky discovery.” Therefore, **serendipity** is a measure of the level of surprise in successful recommendations. In other words, recommendations need to be unexpected. In contrast, novelty only requires that the user was not aware of the recommendation earlier. Serendipity is a stronger condition than novelty. All serendipitious recommendations are novel, but the converse is not always true.\n",
    "\n",
    "There are several ways of measuring serendipity in recommender systems:\n",
    "1. **Online methods:** <br>\n",
    "The recommender system collects user feedback both on the usefulness of a recommendation and its obviousness. The fraction of recommendations that are both useful and non-obvious, is used as a measure of the serendipity.\n",
    "\n",
    "2. **Offline methods:** <br>\n",
    "One can also use a primitive recommender to generate the information about the obviousness of a recommendation in an automated way. The primitive recommender is typically selected as a content-based recommender, which has a high propensity for recommending obvious items. Then, the fraction of the recommended items in the top-$k$ lists that are correct (i.e., high values of hidden ratings), and are also not recommended by the primitive recommender are determined. This fraction provides a measure of the serendipity.\n",
    "\n",
    "It is noteworthy that it is not sufficient to measure the fraction of non-obvious items, because a system might recommend unrelated items. Therefore, the usefulness of the items is always incorporated in the measurement of serendipity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of diversity implies that the set of proposed recommendations within a single recommended list should be as diverse as possible. If all three movies are of a particular genre and contain similar actors, then there is little diversity in the recommendations. If the user dislikes the top choice, then there is a good chance that she might dislike all of them. Presenting different types of movies can often increase the chance that the user might select one of them. Note that the diversity is always measured over a set of recommendations, and it is closely related to novelty and serendipity.\n",
    "\n",
    "Diversity can be measured in terms of the content-centric similarity between pairs of items. The vector-space representation of each item description is used for the similarity computation. For example, if a set of $k$ items are recommended to the user, then the pairwise similarity is computed between every pair of items in the list. The average similarity between all pairs can be reported as the diversity. Lower values of the average similarity indicate greater diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, it has become increasingly easy to collect large numbers of ratings and implicit feedback information from various users. In such cases, the sizes of the data sets continue to increase over time. A variety of measures are used for determining the scalability of a system:\n",
    "\n",
    "1. **Training time:** <br>\n",
    "Most recommender systems require a training phase, which is separate from the testing phase.\n",
    "\n",
    "2. **Prediction time:** <br>\n",
    "Once a model has been trained, it is used to determine the top recommendations for a particular customer. It is crucial for the prediction time to be low, because it determines the latency with which the user receives the responses.\n",
    "\n",
    "3. **Memory requirements:** <br>\n",
    "When the ratings matrices are large, it is sometimes a challenge to hold the entire matrix in the main memory. In such cases, it is essential to design the algorithm to minimize memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Accuracy Metrics in Offline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference of recommender system evaluation compared to classificaton is that in classification the data are segmented on a row-wise basis (between training and test rows), whereas in collaborative filtering the data are segmented on an entry-wise basis (between training and test entries). This difference closely mirrors the nature of the relationship between the classification and the collaborative filtering problems.\n",
    "\n",
    "Also, another difference from classification design is that the performance on hidden entries often does not reflect the true performance of the system in real settings. This is because the hidden ratings are not chosen at random from the matrix. Rather, the hidden ratings are typically items that the user has chosen to consume. Therefore, such entries are likely to have higher values of the ratings as compared to truly missing values. This is a problem of **sample selection bias**. Although this problem could also arise in classification, it is far more pervasive in collaborative filtering applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Accuracy of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\text{MSE} = \\frac{\\sum_{(u, j) \\in E} e_{uj}^{2}}{|E|}$\n",
    "* $\\text{RMSE} = \\sqrt{\\frac{\\sum_{(u, j) \\in E} e_{uj}^{2}}{|E|}}$\n",
    "* $\\text{MAE} = \\frac{\\sum_{(u, j) \\in E} |e_{uj}|}{|E|}$\n",
    "\n",
    "Note: One problem with these metrics is that they are heavily influenced by the ratings on the popular items. The items that receive very few ratings are ignored. Therefore, the prediction accuracies on sparse items will typically be different from those on popular items. One way of handling this problem is to compute the RMSE or MAE separately for all the hidden ratings associated with each item, and then average over the different items in a weighted way. In other words, the accuracy computations of Equations RMSE and MAE can be **weighted with an item-specific weight**, depending on the relative importance, profit, or utility to the merchant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Evaluating Ranking via Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the recommender system creates a ranking of items for a user, and the top-$k$ items are recommended. The value of $k$ may vary with the system, item, and user at hand. Consider a user $u$, for which the ratings of the set $I_u$ of items have been hidden by a hold-out or cross-validation strategy. For example, if the ratings of the first, third, and fifth items (columns) of user (row) $u$ are hidden for evaluation purposes, then we have $I_u = \\{1, 3, 5\\}$. We would like to measure how well the ground-truth orderings of the ratings in $I_u$ are related to the ordering predicted by the recommender system for the set $I_u$. An important issue to keep in mind is that ratings are typically chosen from a discrete scale, and many ties exist in the ground truth.\n",
    "\n",
    "The most common class of methods is to use rank correlation coefficients. The two most commonly used rank correlation coefficients are as follows:\n",
    "1. **Spearman rank correlation coefficient**:\n",
    "The Spearman correlation coefficient is simply equal to the Pearson correlation coefficient applied on these ranks. The computed value always ranges in (−1, +1), and large positive values are more desirable. The Spearman correlation coefficient is specific to user $u$, and it can then be averaged over all users to obtain a global value. Alternatively, the Spearman rank correlation can be computed over all the hidden ratings over all users in one shot, rather than computing user-specific values and averaging them. One problem with this approach is that the ground truth will contain many ties, and therefore random tie-breaking might lead to some noise in the evaluation. For this purpose, an approach referred to as tie-corrected Spearman is used.\n",
    "\n",
    "1. For each pair of items $(j, k) \\in I_u $, the following credit $C(j, k)$ is computed by comparing the predicted ranking with the ground-truth ranking of these items:\n",
    "\n",
    "$$\n",
    "C(j, k) =\n",
    "\\begin{cases}\n",
    "+1 & \\text{if items } j \\text{ and } k \\text{ are in the same relative order in ground-truth ranking and predicted ranking (concordant)} \\\\\n",
    "-1 & \\text{if items } j \\text{ and } k \\text{ are in a different relative order in ground-truth ranking and predicted ranking (discordant)} \\\\\n",
    "0 & \\text{if items } j \\text{ and } k \\text{ are tied in either the ground-truth ranking or predicted ranking.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then, the Kendall rank correlation coefficient $\\tau_u$, which is specific to user $u$, is computed as the average value of $C(j, k)$ over all the $ \\frac{|I_u|(|I_u| - 1)}{2} $ pairs of test items for user $u$:\n",
    "$$\n",
    "\\tau_u = \\frac{\\sum_{j < k} C(j, k)}{|I_u| \\cdot (|I_u| - 1) / 2}.\n",
    "$$\n",
    "\n",
    "A different way of understanding the Kendall rank correlation coefficient is as follows:\n",
    "$$\n",
    "\\tau_u = \\frac{\\text{Number of concordant pairs} - \\text{Number of discordant pairs}}{\\text{Number of pairs in } I_u}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Evaluating Ranking via Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility-based methods use the ground-truth rating in combination with the recommender system’s ranking. The overall goal of utility-based methods is to create a crisp quantification of how useful the customer might find the recommender system’s ranking. An important principle underlying such methods is that recommendation lists are short compared to the total number of items.\n",
    "\n",
    "In utility-based ranking, the basic idea is that each item in $I_u$ has a utility to the user, which depends both on its position in the recommended list and its ground-truth rating. Furthermore, items ranked higher in the recommended list have greater utility to the user $i$ because they are more likely to be noticed (by virtue of their position) and eventually selected. Ideally, one would like items with higher ground-truth rating to be placed as high on the recommendation list as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. R-Score\n",
    "\n",
    "For any item $j ∈ I_u$, its rating-based utility to the user $i$ is assumed to be $\\text{max}\\{r_{uj} − C_u, 0\\}$, where $C_u$ is a break even\n",
    "(neutral) rating value for user $u$. For example, $C_u$ might be set to the mean rating of user $u$. On the other hand, the ranking-based utility of the item is $2^{−\\frac{(v_j−1)}{α}}$, where $v_j$ is the rank of item $j$ in the list of recommended items and $α$ is a half-life parameter.\n",
    "\n",
    "The utility $F(u, j)$ of item $j ∈ I_u$ to user $u$ is defined as the product of the rating-based and ranking-based utility values:\n",
    "$$F(u, j) = \\frac{\\text{max}\\{r_{uj} − C_u, 0\\}}{2^{−\\frac{(v_j−1)}{α}}}$$\n",
    "\n",
    "The $R$-score, which is specific to user $u$, is the sum of $F(u, j)$ over all the hidden ratings in $I_u$:\n",
    "$$\\text{R-Score}(u) = \\sum\\limits_{j \\epsilon I_u} F(u, j)$$\n",
    "\n",
    "Note that the value of $v_j$ can take on any value from $1$ to $n$, where $n$ is the total number of items. However, in practice, one often restricts the size of the recommended list to a maximum value of $L$. One can therefore compute the $R$-score over a recommended list of\n",
    "specific size $L$ instead of using all the items, as follows:\n",
    "\n",
    "$$\\text{R-Score}(u) = \\sum\\limits_{j \\epsilon I_u, v_j \\le L} F(u, j)$$\n",
    "\n",
    "The idea here is that ranks below $L$ have no utility to the user because the recommended list is of size $L$.\n",
    "\n",
    "The overall $R$-score may be computed by summing this value over all the users:\n",
    "$$\\text{R-Score} = \\sum_{u=1}^{m} \\text{R-Score}(u)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3.2. NDCG\n",
    "\n",
    "The exponential decay in the utility implies that users are only interested in top-ranked items, and they do not pay much attention to lower-ranked items. This may not be true in all applications, especially in news recommender systems, where users typically browse multiple items lower down the list of recommended items. In such cases, the discount rate should be set in a milder way. An example of such a measure is the discounted cumulative gain (DCG). In this case, the discount factor of item $j$ is set to $log2(v_j + 1)$, where $v_j$ is the rank of item $j$ in the test set $I_u$. Then, the discounted cumulative gain is defined as follows:\n",
    "$$ \\text{DCG} = \\frac{1}{m} \\sum_{u=1}^{m} \\sum_{j \\epsilon I_u} \\frac{g_{uj}}{log_2(v_j + 1)} $$\n",
    "\n",
    "Here, $g_{uj}$ represents the utility (or gain) of the user $u$ in consuming item $j$. Typically, the value of $g_{uj}$ is set to an exponential function of the relevance (e.g., non-negative ratings or user hit rates):\n",
    "$$ g_{uj} = 2^{\\text{rel}_{uj} - 1}$$\n",
    "\n",
    "Here, $\\text{rel}_{uj}$ is the ground-truth relevance of item $j$ for user $u$, which is computed as a heuristic function of the ratings or hits.\n",
    "\n",
    "In many settings, the raw ratings are used. It is common to compute the discounted cumulative gain over a recommendation list of specific size $L$, rather than using all the items:\n",
    "$$ \\text{DCG} = \\frac{1}{m} \\sum_{u=1}^{m} \\sum\\limits_{j \\epsilon I_u, v_j \\le L} \\frac{g_{uj}}{log_2(v_j + 1)} $$\n",
    "\n",
    "The basic idea is that recommended lists have size no larger than $L$. Then, the normalized discounted cumulative gain (NDCG) is defined as ratio of the discounted cumulative gain to its ideal value, which is also referred to as ideal discounted cumulative gain (IDCG).\n",
    "$$ \\text{NDCG} = \\frac{DCG}{IDCG} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. ARHR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another measure that is commonly used, is the average reciprocal hit rate (ARHR). This measure is designed for implicit feedback data sets, in which each value of $r_{uj} ∈ \\{0, 1\\}$. Therefore, a value of $r_{uj} = 1$ represents a “hit” where a customer has bought or clicked on an item. A value of $r_{uj} = 0$ corresponds to a situation where a customer has not bought or clicked on an item. In this implicit feedback setting, missing values in the ratings matrix are assumed to be 0. In this case, the rank-based discount rate is $\\frac{1}{v_j}$, where $v_j$ is the rank of item $j$ in the recommended list, and the item utility is simply the hidden “rating” value $r_{uj} ∈ \\{0, 1\\}$.\n",
    "\n",
    "The ARHR metric for the user $i$ is defined by summing up these values over all the hidden items in $I_u$:\n",
    "$$ARHR(u) = \\sum_{j \\epsilon I_u}\\frac{r_{uj}}{v_j}$$\n",
    "\n",
    "It is also possible to define the average reciprocal hit-rate for a recommended list of size $L$ by adding only those utility values for which $v_j \\le L$.\n",
    "$$ARHR(u) = \\sum\\limits_{j \\epsilon I_u, v_j \\le L}\\frac{r_{uj}}{v_j}$$\n",
    "\n",
    "One quirk of the average reciprocal hit-rate is that it is typically used when the value of $|I_u|$ is exactly 1, and when the value r_{uj} of the corresponding (hidden) item $j ∈ I_u$ is always 1. Therefore, there is exactly one hidden item for each user, and the user has always bought or clicked on this item. In other words, the average reciprocal hit-rate rewards the utility (in a rank-reciprocal way) for recommending the single correct answer at a high position on the recommended list. \n",
    "\n",
    "The global ARHR value is computed by averaging this value over the $m$ users:\n",
    "$$ARHR = \\frac{\\sum_{u=1}^{m}ARHR(u)}{m}$$\n",
    "\n",
    "The average reciprocal hit-rate is also referred to as the mean reciprocal rank (MRR). In cases where the value of $|I_u|$ is 1, the average reciprocal hit-rate always lies in the range (0, 1). In such cases, the hidden entry is usually an item for which $r_{uj} = 1$ and the length of\n",
    "the recommendation list is restricted to $L$.\n",
    "\n",
    "The $\\text{ARHR}$ and $\\text{HR}$ are almost always used in implicit feedback data sets, in which missing values are treated as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4. MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related measure is the **mean average precision (MAP)**, which computes the fraction of relevant items in a recommended list of length $L$ for a given user. Various equally spaced values of $L$ are used, and the precision is averaged over these recommendation lists of varying lengths. The resulting precision is then averaged over all the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Evaluating Ranking via Receiver Operating Characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The items that are eventually consumed are referred to as the ground-truth positives or true positives. By varying the size of the recommended list, one can then examine the fraction of relevant (ground-truth positive) items in the list, and the fraction of relevant items that are missed by the list. If the recommended list is too small, then the algorithm will miss relevant items (false-negatives). On the other hand, if a very large list is recommended, this will lead to too many spurious recommendations that are never used by the user (false-positives). The problem is that the correct size of the recommendation list is never known exactly in a real scenario. However, the entire trade-off curve can be quantified using a variety of measures, and two algorithms can be compared over the entire trade-off curve. Two examples of such curves are the **precision-recall curve** and the **receiver operating characteristic (ROC) curve.**\n",
    "\n",
    "Assume that one selects the top-$t$ set of ranked items to recommend to the user. For any given value $t$ of the size of the recommended list, the set of recommended items is denoted by $S(t)$. Note that $|S(t)| = t$. Therefore, as $t$ changes, the size of $S(t)$ changes as well. Let $G$ represent the true set of relevant items (ground-truth positives) that are consumed by the user. **Precision** is defined as the percentage of recommended items that truly turn out to be relevant (i.e., consumed by the user).\n",
    "$$\\text{Precision}(t) = 100 · \\frac{|S(t) ∩G|}{|S(t)|}$$\n",
    "\n",
    "The value of $\\text{Precision(t)}$ is not necessarily monotonic in $t$ because both the numerator and denominator may change with $t$ differently. The recall is correspondingly defined as the percentage of ground-truth positives that have been recommended as positive for a list of size $t$.\n",
    "$$Recall(t) = 100 · \\frac{|S(t) ∩ G|}{|G|}$$\n",
    "\n",
    "While a natural trade-off exists between precision and recall, this trade-off is not necessarily monotonic.\n",
    "\n",
    "One way of creating a single measure that summarizes both precision and recall is the **F1-measure**, which is the harmonic mean between the precision and the recall.\n",
    "\n",
    "$$F_1(t) = 2 · \\frac{\\text{Precision(t)} · \\text{Recall(t)}}{\\text{Precision(t)} + \\text{Recall(t)}}$$\n",
    "\n",
    "While the $F_1(t)$ measure provides a better quantification than either precision or recall, it is still dependent on the size $t$ of the recommended list and is therefore still not a complete representation of the trade-off between precision and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way of generating the trade-off in a more intuitive way is through the use of the ROC curve. The true-positive rate, which is the same as the recall, is defined as the percentage of ground-truth positives that have been included in the recommendation list of size $t$.\n",
    "$$\\text{TPR}(t) = \\text{Recall}(t) = 100 · \\frac{|S(t) ∩ G|}{G}$$\n",
    "\n",
    "The false-positive rate **FPR(t)** is the percentage of the falsely reported positives in the recommended list out of the ground-truth negatives (i.e., irrelevant items not consumed by the user). Therefore, if $U$ represents the universe of all items, the ground-truth negative set is given by $(U − G)$, and the falsely reported part in the recommendation list is (S(t)−G). Therefore, the false-positive rate is defined as follows: \n",
    "$$FPR(t) = 100 · \\frac{|S(t)−G|}{|U − G|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "Measures such as ROC and Kendall coefficient, which treat higher and lower ranked items equally, are unable to capture the greater importance of higher ranked items. For example, the relative ranking between two items ranked first and second on the recommendation list is far more important than the relative ranking of two items, which are ranked 100th and 101st on the list. In this context, utility-based measures such as NDCG do a much better job than rank-correlation coefficients or ROC measures at distinguishing between higher-ranked and lower-ranked items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Limitations of Evaluation Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy-based evaluation measures have a number of weaknesses that arise out of selection bias in recommender systems. In particular, the missing entries in a ratings matrix are not random because users have the tendency of rating more popular items. As a result, the accuracy of most recommendation algorithms is different on the more popular items versus the items in the long tail. More generally, the fact that a particular user has chosen not to rate a particular item thus far has a significant impact on what her rating would be if the user were forced to rate all items.\n",
    "\n",
    "There are several solutions to this issue. The simplest solution is to not select the missing ratings at random but to use a model for selecting the test ratings based on their likelihood of being rated in the future. Another solution is to not divide the ratings at random between training and test, but to divide them temporally by using more recent ratings as a part of the test data; indeed, the Netflix Prize contest used more recent ratings in the qualifying set, although some of the recent ratings were also provided as a part of the probe set. An approach that has been used in recent years, is to correct for this bias by modeling the bias in the missing rating distribution within the evaluation measures. Although such an approach has some merits, it does have the drawback that the evaluation process itself now assumes a model of how the ratings behave.\n",
    "\n",
    "**Note:** It is important to realize that these limitations in collaborative filtering evaluation are inherent; the quality of any evaluation system is fundamentally limited by the quality of the available ground truth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movielens-100k-recommender-systems-Tq6dyjba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
