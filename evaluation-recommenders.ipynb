{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evaluation of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **three (3)** primary types of evaluation of recommender systems, corresponding to **[1] user studies**, **[2] online evaluations**, and **[3] offline evaluations** with historical data set.\n",
    "\n",
    "* **User Studies:** <br>\n",
    "In user studies, test subjects are actively recruited, and they are asked to interact with the recommender system to perform specific tasks. Feedback can be collected from the user before and after the interaction, and the system also collects information about their interaction with the recommender system. An important advantage of user studies is that they allow for the collection of information about the user interaction with the system. On the other hand, the active awareness of the user about the testing of the recommender system can often bias their choices and actions. It is also difficult and expensive to recruit large cohorts of users for evaluation purposes. In many cases, the recruited users are not representative of the general population because the recruitment process is itself a bias-centric filter, which cannot be fully controlled.\n",
    "\n",
    "* **Online Evaluation:** <br>\n",
    "Online evaluations also leverage user studies except that the users are often real users in a fully deployed or commercial system. This approach is sometimes less susceptible to bias from the recruitment process, because the users are often directly using the system in the natural course of affairs. Typically, users can be sampled randomly, and the various algorithms can be tested with each sample of users. A typical example of a metric, which is used to measure the effectiveness of the recommender system on the users, is the conversion rate. The main disadvantage is that such systems cannot be realistically deployed unless a large number of users are already enrolled. Therefore, it is hard to use this method during the start up phase.\n",
    "\n",
    "* **Offline Evaluation:** <br>\n",
    "In offline testing, historical data, such as ratings, are used. In some cases, temporal information may also be associated with the ratings, such as the timestamp at which each user has rated the item. The main disadvantage of offline evaluations is that they do not measure the actual propensity of the user to react to the recommender system in the future. For example, the data might evolve over time, and the current predictions may not reflect the most appropriate predictions for the future. Furthermore, measures such as accuracy do not capture important characteristics of recommendations, such as serendipity and novelty. Offline methods continue to be the most widely accepted techniques for recommender system evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. General Goals of Evaluation Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from the well known goal of **accuracy**, other general goals of evaluating a recommender system holistically include factors such as **diversity**, **serendipity**, **novelty**, **robustness**, and **scalability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the most general case, ratings are numeric quantities that need to be estimated. Therefore, the accuracy metrics are often similar to those used in regression modeling. Let $R$ be the ratings matrix in which $r_{uj}$ is the known rating of user $u$ for item $j$. Consider the case where a recommendation algorithm estimates this rating as $\\hat{r}_{uj}$. Then, the entry-specific error of the estimation is given by the quantity $\\epsilon_{uj} = \\hat{r}_{uj} − r_{uj}$. \n",
    "\n",
    "The overall error is computed by averaging the entry-specific errors either in terms of absolute values or in terms of squared values.\n",
    "\n",
    "* $\\text{MSE} = \\frac{\\sum_{(u, j) \\in E} e_{uj}^{2}}{|E|}$\n",
    "\n",
    "* $\\text{RMSE} = \\sqrt{\\frac{\\sum_{(u, j) \\in E} e_{uj}^{2}}{|E|}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many recommender systems do not directly estimate ratings; instead, they provide estimates of the underlying ranks. Depending on the nature of the ground-truth, one can use rank-correlation measures, utility-based measures, or the receiver operating characteristic which will be discussed later.\n",
    "\n",
    "Some measures of accuracy are also designed to maximize the profit for the merchant because all items are not equally important from the perspective of the recommendation process. These metrics incorporate item-specific costs into the computation. The main problem with accuracy metrics is that they often do not measure the true effectiveness of a recommender system in real settings. For example, an obvious recommendation might be accurate, but a user might have eventually bought that item anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when a recommender system is highly accurate, it may often not be able to ever recommend a certain proportion of the items, or it may not be able to ever recommend to a certain proportion of the users. This measure is referred to as **Coverage**. This limitation of recommender systems is an artifact of the fact that ratings matrices are sparse. Therefore, the trade-off between accuracy and coverage always needs to be incorporated into the evaluation process. \n",
    "\n",
    "There are two types of coverage, which are referred to as **[1] user-space** coverage and **[2] item-space** coverage, respectively.\n",
    "\n",
    "* **User-Space Coverage:** <br>\n",
    "**User-space** coverage measures the fraction of users for which at least $k$ ratings may be predicted. The value of $k$ should be set to the expected size of the recommendation list. When fewer than $k$ ratings can be predicted for a user, it is no longer possible to present a meaningful recommendation list of size $k$ to the user. Such a situation could occur when a user has specified very few ratings in common with other users. It is difficult to robustly compute the peers of that user, because of very few mutually specified ratings with other users. Therefore, it is often difficult to make sufficient recommendations for that user. For very high levels of sparsity, it is possible that no algorithm may be able to predict even one rating for that user. \n",
    "\n",
    "*Note:* A tricky aspect of user-space coverage is that any algorithm can provide full coverage by simply predicting random ratings for user-item combinations, whose ratings it cannot reliably predict. Therefore, *user-space coverage should always be evaluated in terms of the trade-off between accuracy and coverage.*\n",
    "\n",
    "* **Item-Space Coverage:** <br>\n",
    "**Item-space** coverage measures the fraction of items for which the ratings of at least $k$ users can be predicted. In practice, however, this notion is rarely used, because recommender systems generally provide recommendation lists for users, and they are only rarely used for generating recommended users for items. \n",
    "\n",
    "A different form of item-space coverage evaluation is defined by the notion of **catalog coverage (CC)**, which is specifically suited to recommendation lists. Let $T_u$ represent the list of top-$k$ items recommended to user $u ∈ \\{1, . . .,m\\}$. The catalog coverage $CC$ is defined as the fraction of items that are recommended to at least one user. $$ CC = \\frac{|\\bigcup_{u=1}^{m}T_u|}{n} $$\n",
    "\n",
    "Here, the notation $n$ represents the number of items. It is easy to estimate this fraction through the use of experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Novelty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **novelty** of a recommender system evaluates the likelihood of a recommender system to give recommendations to the user that they are not aware of, or that they have not seen before. Unseen recommendations often increase the ability of the user to discover important insights into their likes and dislikes that they did not know previously. This is more important than discovering items that they were already aware of but they have not rated. \n",
    "\n",
    "The most natural way of measuring novelty is through online experimentation in which users are explicitly asked whether they were aware of an item previously and, fortunately, it is also possible to approximately estimate novelty with offline methods, as long as timestamps are available with the ratings. The basic idea is that novel systems are better at recommending items that are more likely to be selected by the user in the future, rather than at the present time. Therefore, all ratings that were created after a certain point in time $t_0$ are removed from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Serendipity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word “serendipity” literally means “lucky discovery.” Therefore, **serendipity** is a measure of the level of surprise in successful recommendations. In other words, recommendations need to be unexpected. In contrast, novelty only requires that the user was not aware of the recommendation earlier. Serendipity is a stronger condition than novelty. All serendipitious recommendations are novel, but the converse is not always true.\n",
    "\n",
    "There are several ways of measuring serendipity in recommender systems:\n",
    "1. **Online methods:** <br>\n",
    "The recommender system collects user feedback both on the usefulness of a recommendation and its obviousness. The fraction of recommendations that are both useful and non-obvious, is used as a measure of the serendipity.\n",
    "\n",
    "2. **Offline methods:** <br>\n",
    "One can also use a primitive recommender to generate the information about the obviousness of a recommendation in an automated way. The primitive recommender is typically selected as a content-based recommender, which has a high propensity for recommending obvious items. Then, the fraction of the recommended items in the top-$k$ lists that are correct (i.e., high values of hidden ratings), and are also not recommended by the primitive recommender are determined. This fraction provides a measure of the serendipity.\n",
    "\n",
    "It is noteworthy that it is not sufficient to measure the fraction of non-obvious items, because a system might recommend unrelated items. Therefore, the usefulness of the items is always incorporated in the measurement of serendipity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of diversity implies that the set of proposed recommendations within a single recommended list should be as diverse as possible. If all three movies are of a particular genre and contain similar actors, then there is little diversity in the recommendations. If the user dislikes the top choice, then there is a good chance that she might dislike all of them. Presenting different types of movies can often increase the chance that the user might select one of them. Note that the diversity is always measured over a set of recommendations, and it is closely related to novelty and serendipity.\n",
    "\n",
    "Diversity can be measured in terms of the content-centric similarity between pairs of items. The vector-space representation of each item description is used for the similarity computation. For example, if a set of $k$ items are recommended to the user, then the pairwise similarity is computed between every pair of items in the list. The average similarity between all pairs can be reported as the diversity. Lower values of the average similarity indicate greater diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, it has become increasingly easy to collect large numbers of ratings and implicit feedback information from various users. In such cases, the sizes of the data sets continue to increase over time. A variety of measures are used for determining the scalability of a system:\n",
    "\n",
    "1. **Training time:** <br>\n",
    "Most recommender systems require a training phase, which is separate from the testing phase.\n",
    "\n",
    "2. **Prediction time:** <br>\n",
    "Once a model has been trained, it is used to determine the top recommendations for a particular customer. It is crucial for the prediction time to be low, because it determines the latency with which the user receives the responses.\n",
    "\n",
    "3. **Memory requirements:** <br>\n",
    "When the ratings matrices are large, it is sometimes a challenge to hold the entire matrix in the main memory. In such cases, it is essential to design the algorithm to minimize memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Accuracy Metrics in Offline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference of recommender system evaluation compared to classificaton is that in classification the data are segmented on a row-wise basis (between training and test rows), whereas in collaborative filtering the data are segmented on an entry-wise basis (between training and test entries). This difference closely mirrors the nature of the relationship between the classification and the collaborative filtering problems.\n",
    "\n",
    "Also, another difference from classification design is that the performance on hidden entries often does not reflect the true performance of the system in real settings. This is because the hidden ratings are not chosen at random from the matrix. Rather, the hidden ratings are typically items that the user has chosen to consume. Therefore, such entries are likely to have higher values of the ratings as compared to truly missing values. This is a problem of **sample selection bias**. Although this problem could also arise in classification, it is far more pervasive in collaborative filtering applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Accuracy of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\text{MSE} = \\frac{\\sum_{(u, j) \\in E} e_{uj}^{2}}{|E|}$\n",
    "* $\\text{RMSE} = \\sqrt{\\frac{\\sum_{(u, j) \\in E} e_{uj}^{2}}{|E|}}$\n",
    "* $\\text{MAE} = \\frac{\\sum_{(u, j) \\in E} |e_{uj}|}{|E|}$\n",
    "\n",
    "Note: One problem with these metrics is that they are heavily influenced by the ratings on the popular items. The items that receive very few ratings are ignored. Therefore, the prediction accuracies on sparse items will typically be different from those on popular items. One way of handling this problem is to compute the RMSE or MAE separately for all the hidden ratings associated with each item, and then average over the different items in a weighted way. In other words, the accuracy computations of Equations RMSE and MAE can be **weighted with an item-specific weight**, depending on the relative importance, profit, or utility to the merchant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Evaluating Ranking via Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the recommender system creates a ranking of items for a user, and the top-$k$ items are recommended. The value of $k$ may vary with the system, item, and user at hand. Consider a user $u$, for which the ratings of the set $I_u$ of items have been hidden by a hold-out or cross-validation strategy. For example, if the ratings of the first, third, and fifth items (columns) of user (row) $u$ are hidden for evaluation purposes, then we have $I_u = \\{1, 3, 5\\}$. We would like to measure how well the ground-truth orderings of the ratings in $I_u$ are related to the ordering predicted by the recommender system for the set $I_u$. An important issue to keep in mind is that ratings are typically chosen from a discrete scale, and many ties exist in the ground truth.\n",
    "\n",
    "The most common class of methods is to use rank correlation coefficients. The two most commonly used rank correlation coefficients are as follows:\n",
    "1. **Spearman rank correlation coefficient**:\n",
    "The Spearman correlation coefficient is simply equal to the Pearson correlation coefficient applied on these ranks. The computed value always ranges in (−1, +1), and large positive values are more desirable. The Spearman correlation coefficient is specific to user $u$, and it can then be averaged over all users to obtain a global value. Alternatively, the Spearman rank correlation can be computed over all the hidden ratings over all users in one shot, rather than computing user-specific values and averaging them. One problem with this approach is that the ground truth will contain many ties, and therefore random tie-breaking might lead to some noise in the evaluation. For this purpose, an approach referred to as tie-corrected Spearman is used.\n",
    "\n",
    "1. For each pair of items $(j, k) \\in I_u $, the following credit $C(j, k)$ is computed by comparing the predicted ranking with the ground-truth ranking of these items:\n",
    "\n",
    "$$\n",
    "C(j, k) =\n",
    "\\begin{cases}\n",
    "+1 & \\text{if items } j \\text{ and } k \\text{ are in the same relative order in ground-truth ranking and predicted ranking (concordant)} \\\\\n",
    "-1 & \\text{if items } j \\text{ and } k \\text{ are in a different relative order in ground-truth ranking and predicted ranking (discordant)} \\\\\n",
    "0 & \\text{if items } j \\text{ and } k \\text{ are tied in either the ground-truth ranking or predicted ranking.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then, the Kendall rank correlation coefficient $\\tau_u$, which is specific to user $u$, is computed as the average value of $C(j, k)$ over all the $ \\frac{|I_u|(|I_u| - 1)}{2} $ pairs of test items for user $u$:\n",
    "$$\n",
    "\\tau_u = \\frac{\\sum_{j < k} C(j, k)}{|I_u| \\cdot (|I_u| - 1) / 2}.\n",
    "$$\n",
    "\n",
    "A different way of understanding the Kendall rank correlation coefficient is as follows:\n",
    "$$\n",
    "\\tau_u = \\frac{\\text{Number of concordant pairs} - \\text{Number of discordant pairs}}{\\text{Number of pairs in } I_u}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Evaluating Ranking via Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movielens-100k-recommender-systems-Tq6dyjba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
